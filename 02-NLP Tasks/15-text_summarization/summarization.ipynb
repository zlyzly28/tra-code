{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于T5的文本摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers rouge-score nltk rouge_chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:08.285845Z",
     "iopub.status.busy": "2024-07-23T11:22:08.285524Z",
     "iopub.status.idle": "2024-07-23T11:22:12.386342Z",
     "shell.execute_reply": "2024-07-23T11:22:12.385659Z",
     "shell.execute_reply.started": "2024-07-23T11:22:08.285826Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-23 19:22:10.445675: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-23 19:22:10.484917: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-23 19:22:11.109370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:14.236088Z",
     "iopub.status.busy": "2024-07-23T11:22:14.235347Z",
     "iopub.status.idle": "2024-07-23T11:22:14.244783Z",
     "shell.execute_reply": "2024-07-23T11:22:14.244295Z",
     "shell.execute_reply.started": "2024-07-23T11:22:14.236062Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk(\"./nlpcc_2017/\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:15.162500Z",
     "iopub.status.busy": "2024-07-23T11:22:15.162139Z",
     "iopub.status.idle": "2024-07-23T11:22:15.170877Z",
     "shell.execute_reply": "2024-07-23T11:22:15.170271Z",
     "shell.execute_reply.started": "2024-07-23T11:22:15.162481Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /mnt/workspace/tra-code/02-NLP Tasks/15-text_summarization/nlpcc_2017/cache-f88b03791a18aede.arrow and /mnt/workspace/tra-code/02-NLP Tasks/15-text_summarization/nlpcc_2017/cache-6b0d5568bb085c36.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 4800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'content'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.train_test_split(200, seed=42)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:16.825070Z",
     "iopub.status.busy": "2024-07-23T11:22:16.824700Z",
     "iopub.status.idle": "2024-07-23T11:22:16.829699Z",
     "shell.execute_reply": "2024-07-23T11:22:16.828971Z",
     "shell.execute_reply.started": "2024-07-23T11:22:16.825051Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '郴州市发布雷电橙色预警:过去2小时北湖区、苏仙区、郴州市区、桂阳县、宜章县、嘉禾县、资兴市、桂东县、汝城县已经受...',\n",
       " 'content': '发布日期:2015-03-3007:55:33郴州市气象台3月30日7时52分发布雷电橙色预警信号:过去2小时北湖区、苏仙区、郴州市区、桂阳县、宜章县、嘉禾县、资兴市、桂东县、汝城县已经受雷电活动影响,并将持续,出现雷电灾害事故的可能性比较大,请注意防范。图例标准防御指南2小时内发生雷电活动的可能性很大,或者已经受雷电活动影响,且可能持续,出现雷电灾害事故的可能性比较大。1、政府及相关部门按照职责落实防雷应急措施;2、人员应当留在室内,并关好门窗;3、户外人员应当躲入有防雷设施的建筑物或者汽车内;4、切断危险电源,不要在树下、电杆下、塔吊下避雨;5、在空旷场地不要打伞,不要把农具、羽毛球拍、高尔夫球杆等扛在肩上。'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:18.977427Z",
     "iopub.status.busy": "2024-07-23T11:22:18.977107Z",
     "iopub.status.idle": "2024-07-23T11:22:20.120008Z",
     "shell.execute_reply": "2024-07-23T11:22:20.119395Z",
     "shell.execute_reply.started": "2024-07-23T11:22:18.977404Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 19:22:19,001 - modelscope - INFO - PyTorch version 2.0.1+cu118 Found.\n",
      "2024-07-23 19:22:19,003 - modelscope - INFO - TensorFlow version 2.13.0 Found.\n",
      "2024-07-23 19:22:19,004 - modelscope - INFO - Loading ast index from /mnt/workspace/.cache/modelscope/ast_indexer\n",
      "2024-07-23 19:22:19,026 - modelscope - INFO - Loading done! Current index file version is 1.9.5, with md5 e096d041fdba41d8d7d7331b3f469288 and a total number of 945 components indexed\n",
      "2024-07-23 19:22:19,731 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download(\"langboat/mengzi-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:21.204654Z",
     "iopub.status.busy": "2024-07-23T11:22:21.204320Z",
     "iopub.status.idle": "2024-07-23T11:22:21.208162Z",
     "shell.execute_reply": "2024-07-23T11:22:21.207619Z",
     "shell.execute_reply.started": "2024-07-23T11:22:21.204635Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/workspace/.cache/modelscope/langboat/mengzi-t5-base'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:22.739776Z",
     "iopub.status.busy": "2024-07-23T11:22:22.739421Z",
     "iopub.status.idle": "2024-07-23T11:22:22.926524Z",
     "shell.execute_reply": "2024-07-23T11:22:22.925993Z",
     "shell.execute_reply.started": "2024-07-23T11:22:22.739757Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:25.105989Z",
     "iopub.status.busy": "2024-07-23T11:22:25.105671Z",
     "iopub.status.idle": "2024-07-23T11:22:25.109505Z",
     "shell.execute_reply": "2024-07-23T11:22:25.108966Z",
     "shell.execute_reply.started": "2024-07-23T11:22:25.105971Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_func(exmaples):\n",
    "    contents = [\"摘要生成: \\n\" + e for e in exmaples[\"content\"]]\n",
    "    inputs = tokenizer(contents, max_length=384, truncation=True)\n",
    "    labels = tokenizer(text_target=exmaples[\"title\"], max_length=64, truncation=True)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:26.153793Z",
     "iopub.status.busy": "2024-07-23T11:22:26.153444Z",
     "iopub.status.idle": "2024-07-23T11:22:26.173098Z",
     "shell.execute_reply": "2024-07-23T11:22:26.172515Z",
     "shell.execute_reply.started": "2024-07-23T11:22:26.153775Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /mnt/workspace/tra-code/02-NLP Tasks/15-text_summarization/nlpcc_2017/cache-1b523153b2e090f8.arrow\n",
      "Loading cached processed dataset at /mnt/workspace/tra-code/02-NLP Tasks/15-text_summarization/nlpcc_2017/cache-a4ea025e3ee8adf8.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 4800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'content', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(process_func, batched=True)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:28.042051Z",
     "iopub.status.busy": "2024-07-23T11:22:28.041731Z",
     "iopub.status.idle": "2024-07-23T11:22:28.047618Z",
     "shell.execute_reply": "2024-07-23T11:22:28.046902Z",
     "shell.execute_reply.started": "2024-07-23T11:22:28.042033Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'摘要生成: 发布日期:2015-03-3007:55:33郴州市气象台3月30日7时52分发布雷电橙色预警信号:过去2小时北湖区、苏仙区、郴州市区、桂阳县、宜章县、嘉禾县、资兴市、桂东县、汝城县已经受雷电活动影响,并将持续,出现雷电灾害事故的可能性比较大,请注意防范。图例标准防御指南2小时内发生雷电活动的可能性很大,或者已经受雷电活动影响,且可能持续,出现雷电灾害事故的可能性比较大。1、政府及相关部门按照职责落实防雷应急措施;2、人员应当留在室内,并关好门窗;3、户外人员应当躲入有防雷设施的建筑物或者汽车内;4、切断危险电源,不要在树下、电杆下、塔吊下避雨;5、在空旷场地不要打伞,不要把农具、羽毛球拍、高尔夫球杆等扛在肩上。</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:29.734174Z",
     "iopub.status.busy": "2024-07-23T11:22:29.733840Z",
     "iopub.status.idle": "2024-07-23T11:22:29.738764Z",
     "shell.execute_reply": "2024-07-23T11:22:29.738152Z",
     "shell.execute_reply.started": "2024-07-23T11:22:29.734156Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'郴州市发布雷电橙色预警:过去2小时北湖区、苏仙区、郴州市区、桂阳县、宜章县、嘉禾县、资兴市、桂东县、汝城县已经受...</s>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_ds[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:30.617599Z",
     "iopub.status.busy": "2024-07-23T11:22:30.617245Z",
     "iopub.status.idle": "2024-07-23T11:22:30.621367Z",
     "shell.execute_reply": "2024-07-23T11:22:30.620692Z",
     "shell.execute_reply.started": "2024-07-23T11:22:30.617581Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 27508, 5045, 1299, 1098, 580, 21296, 7701, 13, 888, 99, 1429, 512, 1080, 159, 6, 1193, 1707, 159, 6, 27508, 5045, 159, 6, 4449, 12817, 6, 2471, 761, 308, 6, 2351, 10099, 308, 6, 2476, 1345, 157, 6, 4449, 301, 308, 6, 7787, 17442, 147, 425, 1542, 1]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds[\"train\"][0][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:32.007765Z",
     "iopub.status.busy": "2024-07-23T11:22:32.007438Z",
     "iopub.status.idle": "2024-07-23T11:22:34.219732Z",
     "shell.execute_reply": "2024-07-23T11:22:34.219154Z",
     "shell.execute_reply.started": "2024-07-23T11:22:32.007747Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 创建评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T10:53:25.265956Z",
     "iopub.status.busy": "2024-07-23T10:53:25.265684Z",
     "iopub.status.idle": "2024-07-23T10:53:25.268335Z",
     "shell.execute_reply": "2024-07-23T10:53:25.267889Z",
     "shell.execute_reply.started": "2024-07-23T10:53:25.265940Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install rouge_chinese transformers==4.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:35.833095Z",
     "iopub.status.busy": "2024-07-23T11:22:35.832658Z",
     "iopub.status.idle": "2024-07-23T11:22:36.204118Z",
     "shell.execute_reply": "2024-07-23T11:22:36.203541Z",
     "shell.execute_reply.started": "2024-07-23T11:22:35.833070Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rouge_chinese import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def compute_metric(evalPred):\n",
    "    def calculate_bleu_scores(candidate, references):\n",
    "        candidate = list(candidate.split(\" \"))\n",
    "        reference = [list(references.split(\" \"))]\n",
    "        weights_list = [\n",
    "            (1, 0, 0, 0),   # BLEU-1\n",
    "            (0.5, 0.5, 0, 0),   # BLEU-2\n",
    "            (1/3, 1/3, 1/3, 0),   # BLEU-3\n",
    "            (0.25, 0.25, 0.25, 0.25) # BLEU-4\n",
    "        ]\n",
    "\n",
    "        bleu_scores = []\n",
    "        for weights in weights_list:\n",
    "            # print(sentence_bleu(reference, candidate, weights=weights))\n",
    "            score = sentence_bleu(reference, candidate, weights=weights)\n",
    "            bleu_scores.append(score)\n",
    "        return bleu_scores\n",
    "    predictions, labels = evalPred\n",
    "    decode_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decode_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decode_preds = [\" \".join(p) for p in decode_preds]\n",
    "    decode_labels = [\" \".join(l) for l in decode_labels]\n",
    "    rouge_scores = rouge.get_scores(decode_preds, decode_labels, avg=True)\n",
    "    bleu_scores_batch = np.mean(np.array([calculate_bleu_scores(cand, refs) for cand, refs in zip(decode_preds, decode_labels)]), axis=0)\n",
    "    return {\n",
    "        \"rouge-1\": rouge_scores[\"rouge-1\"][\"f\"],\n",
    "        \"rouge-2\": rouge_scores[\"rouge-2\"][\"f\"],\n",
    "        \"rouge-l\": rouge_scores[\"rouge-l\"][\"f\"],\n",
    "        \"bleu-l\": bleu_scores_batch[0],\n",
    "        \"bleu-2\": bleu_scores_batch[1],\n",
    "        \"bleu-3\": bleu_scores_batch[2],\n",
    "        \"bleu-4\": bleu_scores_batch[3],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T10:53:25.607659Z",
     "iopub.status.busy": "2024-07-23T10:53:25.607425Z",
     "iopub.status.idle": "2024-07-23T10:53:25.616774Z",
     "shell.execute_reply": "2024-07-23T10:53:25.616095Z",
     "shell.execute_reply.started": "2024-07-23T10:53:25.607645Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['这 是 一 份 行 动 指 南 ， 确 保 军 队 始 终 服 从 党 的 命 令', '确 保 军 队 始 终 服 从 党 的 命 令'] ['这 是 一 份 行 动 指 南 ， 确 保 军 队 永 远 听 从 党 的 指 挥', '确 保 军 队 永 远 听 从 党 的 指 挥']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.6916666666666667,\n",
       "  'p': 0.6726190476190477,\n",
       "  'f': 0.6819105641071784},\n",
       " 'rouge-2': {'r': 0.5772727272727273,\n",
       "  'p': 0.5772727272727273,\n",
       "  'f': 0.5772727222727273},\n",
       " 'rouge-l': {'r': 0.6726190476190477,\n",
       "  'p': 0.6726190476190477,\n",
       "  'f': 0.6726190426190477}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "decode_preds = [\" \".join(p) for p in [\"这是一份行动指南，确保军队始终服从党的命令\", \"确保军队始终服从党的命令\"]]\n",
    "decode_labels = [\" \".join(l) for l in [\"这是一份行动指南，确保军队永远听从党的指挥\", \"确保军队永远听从党的指挥\"]]\n",
    "print(decode_preds, decode_labels)\n",
    "scores = rouge.get_scores(decode_preds, decode_labels, avg=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T10:53:25.617743Z",
     "iopub.status.busy": "2024-07-23T10:53:25.617503Z",
     "iopub.status.idle": "2024-07-23T10:53:25.989958Z",
     "shell.execute_reply": "2024-07-23T10:53:25.988989Z",
     "shell.execute_reply.started": "2024-07-23T10:53:25.617724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_bleu_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 定义一个函数来计算所有四个BLEU分数\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 使用一个列表推导式来计算batch中每个候选句子的BLEU分数\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m bleu_scores_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray([calculate_bleu_scores(cand, refs) \u001b[38;5;28;01mfor\u001b[39;00m cand, refs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(decode_preds, decode_labels)]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m bleu_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbleu-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: score \u001b[38;5;28;01mfor\u001b[39;00m i, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(bleu_scores_batch)}\n\u001b[1;32m     10\u001b[0m bleu_dict\n",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 定义一个函数来计算所有四个BLEU分数\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 使用一个列表推导式来计算batch中每个候选句子的BLEU分数\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m bleu_scores_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray([\u001b[43mcalculate_bleu_scores\u001b[49m(cand, refs) \u001b[38;5;28;01mfor\u001b[39;00m cand, refs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(decode_preds, decode_labels)]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m bleu_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbleu-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: score \u001b[38;5;28;01mfor\u001b[39;00m i, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(bleu_scores_batch)}\n\u001b[1;32m     10\u001b[0m bleu_dict\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_bleu_scores' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "# 定义一个函数来计算所有四个BLEU分数\n",
    "\n",
    "\n",
    "\n",
    "# 使用一个列表推导式来计算batch中每个候选句子的BLEU分数\n",
    "bleu_scores_batch = np.mean(np.array([calculate_bleu_scores(cand, refs) for cand, refs in zip(decode_preds, decode_labels)]), axis=0)\n",
    "bleu_dict = {f'bleu-{i+1}': score for i, score in enumerate(bleu_scores_batch)}\n",
    "bleu_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:44.331458Z",
     "iopub.status.busy": "2024-07-23T11:22:44.331107Z",
     "iopub.status.idle": "2024-07-23T11:22:44.704519Z",
     "shell.execute_reply": "2024-07-23T11:22:44.703997Z",
     "shell.execute_reply.started": "2024-07-23T11:22:44.331437Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers import Seq2SeqTrainingArguments, TrainerCallback\n",
    "from transformers.trainer_utils import TrainOutput\n",
    "\n",
    "class MetricsLoggerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.metrics = []\n",
    "        self.losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            loss = logs['loss']\n",
    "            self.losses.append(loss)\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            self.metrics.append(metrics)\n",
    "\n",
    "# 创建MetricsLoggerCallback实例\n",
    "metrics_logger_callback = MetricsLoggerCallback()\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./summary\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=10,\n",
    "    logging_steps=6,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"rouge-l\",\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(Seq2SeqTrainingArguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step7 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:50.964196Z",
     "iopub.status.busy": "2024-07-23T11:22:50.963861Z",
     "iopub.status.idle": "2024-07-23T11:22:52.860746Z",
     "shell.execute_reply": "2024-07-23T11:22:52.860173Z",
     "shell.execute_reply.started": "2024-07-23T11:22:50.964178Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    compute_metrics=compute_metric,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step8 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:55.561455Z",
     "iopub.status.busy": "2024-07-23T11:22:55.561077Z",
     "iopub.status.idle": "2024-07-23T11:22:55.564750Z",
     "shell.execute_reply": "2024-07-23T11:22:55.564041Z",
     "shell.execute_reply.started": "2024-07-23T11:22:55.561423Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T11:22:57.348699Z",
     "iopub.status.busy": "2024-07-23T11:22:57.348213Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 33/360 01:26 < 15:07, 0.36 it/s, Epoch 0.27/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Bleu-l</th>\n",
       "      <th>Bleu-2</th>\n",
       "      <th>Bleu-3</th>\n",
       "      <th>Bleu-4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6.125700</td>\n",
       "      <td>3.840915</td>\n",
       "      <td>0.306185</td>\n",
       "      <td>0.146935</td>\n",
       "      <td>0.234592</td>\n",
       "      <td>0.188793</td>\n",
       "      <td>0.128804</td>\n",
       "      <td>0.081516</td>\n",
       "      <td>0.053708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.890700</td>\n",
       "      <td>3.048959</td>\n",
       "      <td>0.335438</td>\n",
       "      <td>0.178075</td>\n",
       "      <td>0.269290</td>\n",
       "      <td>0.202171</td>\n",
       "      <td>0.147336</td>\n",
       "      <td>0.100510</td>\n",
       "      <td>0.070499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.490000</td>\n",
       "      <td>2.841954</td>\n",
       "      <td>0.391786</td>\n",
       "      <td>0.221671</td>\n",
       "      <td>0.317150</td>\n",
       "      <td>0.242550</td>\n",
       "      <td>0.183743</td>\n",
       "      <td>0.133815</td>\n",
       "      <td>0.103092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.195500</td>\n",
       "      <td>2.726521</td>\n",
       "      <td>0.410254</td>\n",
       "      <td>0.238629</td>\n",
       "      <td>0.333803</td>\n",
       "      <td>0.247245</td>\n",
       "      <td>0.190688</td>\n",
       "      <td>0.142789</td>\n",
       "      <td>0.110246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.015000</td>\n",
       "      <td>2.653141</td>\n",
       "      <td>0.423811</td>\n",
       "      <td>0.257014</td>\n",
       "      <td>0.348157</td>\n",
       "      <td>0.266004</td>\n",
       "      <td>0.207881</td>\n",
       "      <td>0.160896</td>\n",
       "      <td>0.127823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.8/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=./summary/runs/Jul23_19-22-44_dsw-583028-7856767748-clzf6 --export_scalars_to_csv=./summary/runs/scalars.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T11:28:13.664845Z",
     "iopub.status.busy": "2024-07-23T11:28:13.664515Z",
     "iopub.status.idle": "2024-07-23T11:28:13.728328Z",
     "shell.execute_reply": "2024-07-23T11:28:13.727626Z",
     "shell.execute_reply.started": "2024-07-23T11:28:13.664827Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "/summary/runs/Jul23_19-22-44_dsw-583028-7856767748-clzf6/events.out.tfevents.1721733777.dsw-583028-7856767748-clzf6.1957.0; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 假设你有一个事件文件路径\u001b[39;00m\n\u001b[1;32m     26\u001b[0m event_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/summary/runs/Jul23_19-22-44_dsw-583028-7856767748-clzf6/events.out.tfevents.1721733777.dsw-583028-7856767748-clzf6.1957.0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 28\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_scalars_from_event_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# save_to_csv(data, './summary/runs/scalars.csv')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m, in \u001b[0;36mextract_scalars_from_event_file\u001b[0;34m(event_file_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_scalars_from_event_file\u001b[39m(event_file_path):\n\u001b[0;32m----> 5\u001b[0m     ea \u001b[38;5;241m=\u001b[39m \u001b[43mEventAccumulator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     ea\u001b[38;5;241m.\u001b[39mReload()\n\u001b[1;32m      8\u001b[0m     wall_time \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorboard/backend/event_processing/event_accumulator.py:312\u001b[0m, in \u001b[0;36mEventAccumulator.__init__\u001b[0;34m(self, path, size_guidance, compression_bps, purge_orphaned_data)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator_mutex \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator \u001b[38;5;241m=\u001b[39m \u001b[43m_GeneratorFromPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression_bps \u001b[38;5;241m=\u001b[39m compression_bps\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpurge_orphaned_data \u001b[38;5;241m=\u001b[39m purge_orphaned_data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorboard/backend/event_processing/event_accumulator.py:945\u001b[0m, in \u001b[0;36m_GeneratorFromPath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath must be a valid string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m io_wrapper\u001b[38;5;241m.\u001b[39mIsSummaryEventsFile(path):\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevent_file_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLegacyEventFileLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m directory_watcher\u001b[38;5;241m.\u001b[39mDirectoryWatcher(\n\u001b[1;32m    948\u001b[0m         path,\n\u001b[1;32m    949\u001b[0m         event_file_loader\u001b[38;5;241m.\u001b[39mLegacyEventFileLoader,\n\u001b[1;32m    950\u001b[0m         io_wrapper\u001b[38;5;241m.\u001b[39mIsSummaryEventsFile,\n\u001b[1;32m    951\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorboard/backend/event_processing/event_file_loader.py:136\u001b[0m, in \u001b[0;36mRawEventFileLoader.__init__\u001b[0;34m(self, file_path, detect_file_replacement)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_file_replacement \u001b[38;5;241m=\u001b[39m detect_file_replacement\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[43m_make_tf_record_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_file_replacement \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreopen\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile replacement detection requested, but not enabled because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF record iterator impl does not support reopening. This \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality requires TensorFlow 2.9+\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorboard/backend/event_processing/event_file_loader.py:82\u001b[0m, in \u001b[0;36m_make_tf_record_iterator\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# TODO(#1711): Find non-deprecated replacement for tf_record_iterator.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _silence_deprecation_warnings():\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_record_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:371\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m         _PRINTED_WARNING[\u001b[38;5;28mcls\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) is deprecated and will be removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, _call_location(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date),\n\u001b[1;32m    370\u001b[0m         instructions)\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/lib/io/tf_record.py:167\u001b[0m, in \u001b[0;36mtf_record_iterator\u001b[0;34m(path, options)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"An iterator that read the records from a TFRecords file.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m  IOError: If `path` cannot be opened for reading.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m compression_type \u001b[38;5;241m=\u001b[39m TFRecordOptions\u001b[38;5;241m.\u001b[39mget_compression_type_string(options)\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pywrap_record_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecordIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /summary/runs/Jul23_19-22-44_dsw-583028-7856767748-clzf6/events.out.tfevents.1721733777.dsw-583028-7856767748-clzf6.1957.0; No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "def extract_scalars_from_event_file(event_file_path):\n",
    "    ea = EventAccumulator(event_file_path)\n",
    "    ea.Reload()\n",
    "\n",
    "    wall_time = []\n",
    "    step_nums = []\n",
    "    values = []\n",
    "\n",
    "    for tag in ea.Tags()['scalars']:\n",
    "        for scalar_event in ea.Scalars(tag):\n",
    "            wall_time.append(scalar_event.wall_time)\n",
    "            step_nums.append(scalar_event.step)\n",
    "            values.append(scalar_event.value)\n",
    "\n",
    "    return {'wall_time': wall_time, 'step': step_nums, 'value': values}\n",
    "\n",
    "def save_to_csv(data, file_name):\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# 假设你有一个事件文件路径\n",
    "event_file_path = '/summary/runs/Jul23_19-22-44_dsw-583028-7856767748-clzf6/events.out.tfevents.1721733777.dsw-583028-7856767748-clzf6.1957.0'\n",
    "\n",
    "data = extract_scalars_from_event_file(event_file_path)\n",
    "# save_to_csv(data, './summary/runs/scalars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step9 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-23T10:53:25.996168Z",
     "iopub.status.idle": "2024-07-23T10:53:25.996499Z",
     "shell.execute_reply": "2024-07-23T10:53:25.996351Z",
     "shell.execute_reply.started": "2024-07-23T10:53:25.996337Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-23T10:53:25.997255Z",
     "iopub.status.idle": "2024-07-23T10:53:25.997612Z",
     "shell.execute_reply": "2024-07-23T10:53:25.997494Z",
     "shell.execute_reply.started": "2024-07-23T10:53:25.997482Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-23T10:53:25.998248Z",
     "iopub.status.idle": "2024-07-23T10:53:25.998518Z",
     "shell.execute_reply": "2024-07-23T10:53:25.998423Z",
     "shell.execute_reply.started": "2024-07-23T10:53:25.998413Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe(\"摘要生成:\\n\" + ds[\"test\"][-1][\"content\"], max_length=64, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-23T10:53:25.999118Z",
     "iopub.status.idle": "2024-07-23T10:53:25.999407Z",
     "shell.execute_reply": "2024-07-23T10:53:25.999310Z",
     "shell.execute_reply.started": "2024-07-23T10:53:25.999301Z"
    }
   },
   "outputs": [],
   "source": [
    "ds[\"test\"][-1][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
